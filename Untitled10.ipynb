{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj1-EVT7ngc0"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Embedding,Dense, LSTM, Bidirectional,RepeatVector, GRU, Dropout, TimeDistributed\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "import re\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Algfavj2nqJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df  = pd.read_csv"
      ],
      "metadata": {
        "id": "7v5Zm94QnrUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "0YrJFTKGnyDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "W8r4OM8Vn3l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "345iU-fYn6Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng = df['English words/sentences']\n",
        "fra = df['French words/sentences']"
      ],
      "metadata": {
        "id": "QwjvCtEPn6Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_word_counter = Counter([word for sentence in eng for word in sentence.split()])\n",
        "print('Total count of English words: ', len([word for sentence in eng for word in sentence.split()]))\n",
        "print('Count of Distinct English words: ',len(eng_word_counter))\n",
        "print('10 Most Common words in English: ',list(zip(*eng_word_counter.most_common(10)))[0])"
      ],
      "metadata": {
        "id": "1G7FFMIpn96B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fra_word_counter = Counter([word for sentence in fra for word in sentence.split()])\n",
        "print('Total count of French words: ', len([word for sentence in fra for word in sentence.split()]))\n",
        "print('Count of distinct French wors: ',len(fra_word_counter))\n",
        "print('10 most common words in French: ',list(zip(*fra_word_counter.most_common(10)))[0])"
      ],
      "metadata": {
        "id": "XMAFad9RoAV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(string):\n",
        "    string = string.replace(\"\\u202f\",\" \")\n",
        "    string = string.lower()\n",
        "    \n",
        "    for p in punctuation + \"«»\" + \"0123456789\":\n",
        "        string  =  string.replace(p,\" \")\n",
        "        \n",
        "    string = re.sub('\\s+',' ',string)\n",
        "    return string"
      ],
      "metadata": {
        "id": "SYmioWnHoC0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng = eng.apply(lambda x:clean(x))\n",
        "fra = fra.apply(lambda x:clean(x))"
      ],
      "metadata": {
        "id": "iZ_mPXOioFtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,12))\n",
        "wc= WordCloud(width=600, height =300).generate(' '.join(eng))\n",
        "plt.imshow(wc)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "fTPeLibkoH9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,12))\n",
        "wcf = WordCloud(width=600,height=300).generate(' '.join(fra))\n",
        "plt.imshow(wcf)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "p4ZJcd2HoKcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_count(line):\n",
        "    return len(line.split())"
      ],
      "metadata": {
        "id": "yZ0k-RC3oNkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['English_word_count'] = df['English words/sentences'].apply(lambda x: word_count(x))\n",
        "df['French_word_count'] = df['French words/sentences'].apply(lambda x: word_count(x))"
      ],
      "metadata": {
        "id": "EI_cvmtZoQ19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=1,ncols=2)\n",
        "sns.distplot(df['English_word_count'],ax=axes[0])\n",
        "sns.distplot(df['French_word_count'],ax=axes[1])\n",
        "sns.despine()\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "jy7Aa0x5oTAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEXT PREPROCESSING FUNCTIONS FOR MODEL TRAINING\n",
        "\n",
        "#Tokenizing Text \n",
        "def create_tokenizer(sentences):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "#Finding the maximum sentence length of a language text\n",
        "def max_sentence_length(lines):\n",
        "    return max(len(sentence.split()) for sentence in lines)\n",
        "\n",
        "\n",
        "#Token sequencing and Padding\n",
        "def encode_sequences(tokenizer,sentences,max_sent_len):\n",
        "    text_to_seq = tokenizer.texts_to_sequences(sentences)\n",
        "    text_pad_sequences = pad_sequences(text_to_seq, maxlen = max_sent_len, padding='pre')\n",
        "    return text_pad_sequences"
      ],
      "metadata": {
        "id": "mHTCGt8AoVQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = create_tokenizer(eng)\n",
        "eng_vocab_size = len(eng_tokenizer.word_index)+1\n",
        "max_eng_sent_len = max_sentence_length(eng)\n",
        "print('ENGLISH :')\n",
        "print('Maximum length of sentence in English :', max_eng_sent_len)\n",
        "print('English text vocabulary size :', eng_vocab_size)\n",
        "print('--------------------------------------------')\n",
        "\n",
        "#For French Text - Tokenizer\n",
        "fra_tokenizer = create_tokenizer(fra)\n",
        "fra_vocab_size= len(fra_tokenizer.word_index)+1\n",
        "max_fra_sent_len = max_sentence_length(fra)\n",
        "print('FRENCH :')\n",
        "print('Maximum length of sentence in French :',max_fra_sent_len)\n",
        "print('French text vocabulary size :',fra_vocab_size)"
      ],
      "metadata": {
        "id": "8Zl2v_hsoYJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_eng_sent_len = 25\n",
        "max_fra_sent_len = 25"
      ],
      "metadata": {
        "id": "UpIwdMH5ocdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = encode_sequences(eng_tokenizer, eng, max_eng_sent_len)\n",
        "y = encode_sequences(fra_tokenizer, fra, max_fra_sent_len)"
      ],
      "metadata": {
        "id": "s9Kq8MTFoeij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(inp_vocab_size, out_vocab_size, inp_maxlen, out_maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(inp_vocab_size, 512,input_length = inp_maxlen, mask_zero=True))\n",
        "    model.add(LSTM(512))\n",
        "    model.add(RepeatVector(out_maxlen))\n",
        "    model.add(LSTM(512,return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(1024,activation='relu')))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(TimeDistributed(Dense(out_vocab_size,activation='softmax')))\n",
        "    return model"
      ],
      "metadata": {
        "id": "DQ34iAJNogv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(eng_vocab_size, fra_vocab_size, max_eng_sent_len, max_fra_sent_len)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "PmkAfxXAojl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=SparseCategoricalCrossentropy(),optimizer='adamax',metrics='accuracy')\n",
        "es = EarlyStopping(monitor='val_accuracy',patience=5,mode='max',verbose=1)\n",
        "lr = ReduceLROnPlateau(monitor='val_accuracy',patience=3,mode='max',verbose=1,factor=0.1,min_lr=0.001)\n",
        "history = model.fit(X_train,\n",
        "                    y_train.reshape(y_train.shape[0],y_train.shape[1],1),\n",
        "                    epochs=6,\n",
        "                    batch_size=512,\n",
        "                    callbacks=[es,lr],\n",
        "                    validation_data = (X_test,y_test.reshape(y_test.shape[0],y_test.shape[1],1))\n",
        "                   )"
      ],
      "metadata": {
        "id": "oDuJqLX1omKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xeVRn8J5opMv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}